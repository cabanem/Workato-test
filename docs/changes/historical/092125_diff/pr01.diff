diff --git a/connectors/vertex-ai.rb b/connectors/vertex-ai.rb
@@
-            { name: 'private_key', ontrol_type: 'password',  multiline: true, optional: false,
+            { name: 'private_key', control_type: 'password',  multiline: true, optional: false,
@@
-      execute: lambda do |connection, input, _eis, _eos|
-        # Accepts prepared prompts from RAG_Utils
-        # Validate model
-        call('validate_publisher_model!', connection, input['model'])
-
-        # Build payload - check for prepared input from RAG_Utils
-        payload = if input['formatted_prompt'].present?
-          # Use prepared prompt directly (from RAG_Utils)
-          input['formatted_prompt']
-        else
-          # Build payload using existing method (backward compatibility)
-          call('payload_for_send_message', input)
-        end
+      execute: lambda do |connection, input, _eis, _eos|
+        call('validate_publisher_model!', connection, input['model'])
+        payload = input['formatted_prompt'].presence || call('build_ai_payload', :send_message, input)
@@
-        call('extract_response', response, { type: :generic, json_response: false })
+        call('extract_response', response, { type: :generic, json_response: false })
@@
-        return standard_error_response(type, ratings) if ratings.blank?
+        return call('standard_error_response', type, ratings) if ratings.blank?
@@
-      resp = get(url).
-        params(
-          page_size: 500,
-          page_token: page_token,
-          view: 'PUBLISHER_MODEL_VIEW_BASIC'
-        ).
+      resp = get(url).
+        params(
+          pageSize: 500,
+          pageToken: page_token,
+          view: 'PUBLISHER_MODEL_VIEW_BASIC'
+        ).
@@
-      models = resp['models'] || []
+      models = resp['publisherModels'] || []
@@
-      begin
-        cached_data = workato.cache.get(cache_key)
-        if cached_data.present?
-          # Check if cache is still fresh (we'll cache for 1 hour)
-          cache_time = Time.parse(cached_data['cached_at'])
+      begin
+        cached_data = workato.cache.get(cache_key)
+        if cached_data.present?
+          cache_time = (Time.parse(cached_data['cached_at']) rescue nil)
+          break if cache_time.nil?
           if cache_time > 1.hour.ago
             puts "Using cached model list (#{cached_data['models'].length} models, cached #{((Time.now - cache_time) / 60).round} minutes ago)"
             return cached_data['models']
           else
             puts "Model cache expired, refreshing..."
           end
         end
@@
-      parts = []
+      parts = []
@@
-    end,
+    end,
@@
-    to_model_options: lambda do |models, bucket:, include_preview: false|
+    to_model_options: lambda do |models, bucket:, include_preview: false|
       return [] if models.blank?
@@
-      # Extract unique model IDs efficiently
-      seen_ids = Set.new
-      unique_models = filtered.select do |m|
-        id = m['name'].to_s.split('/').last
-        seen_ids.add?(id)  # Returns true if added (wasn't present), false if already present
-      end
+      seen_ids = {}
+      unique_models = filtered.select do |m|
+        id = m['name'].to_s.split('/').last
+        next false if id.blank?
+        next false if seen_ids[id]
+        seen_ids[id] = true
+      end
@@
-    batch_fetch_drive_files: {
+    batch_fetch_drive_files: {
@@
-        file_ids.each_slice(5) do |batch| # process 5 files concurrently
-        #file_ids.each do |file_id_input| # simple sequential processing
-          begin
-            # Extract clean file ID
-            file_id = call('extract_drive_file_id', file_id_input)
+        file_ids.each do |file_id_input|
+          begin
+            file_id = call('extract_drive_file_id', file_id_input)
@@
-      # Extract inputs
+      # Extract inputs
@@
-        # Make rate-limited request
-        response = call('rate_limited_ai_request', connection, model, 'embedding', url, payload)
+        response = call('rate_limited_ai_request', connection, model, 'embedding', url, payload)
@@
-        # Return single embedding result
+        # Return single embedding result
         {
           'vector' => vector,
           'dimensions' => vector.length,
           'model_used' => model,
-          'token_count' => token_count,
-          'rate_limit_status' => rate_limit_info
+          'token_count' => token_count,
+          'rate_limit_status' => (response.is_a?(Hash) ? response['rate_limit_status'] : nil)
         }
@@
-            { name: 'question', label: 'Instruction', ptional: false, group: 'Instruction',
+            { name: 'question', label: 'Instruction', optional: false, group: 'Instruction',
